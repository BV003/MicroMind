# MicroMind

## 🚀 Introduction

> This project, MicroMind, is primarily based on and inspired by the MiniMind project by Jingyao Gong (https://github.com/jingyaogong/minimind), with modifications and enhancements specific to this repository.

MicroMind is a lightweight small language model framework, designed for experimentation, research, and learning. Inspired by the MiniMind project by Jingyao Gong, MicroMind introduces enhancements and optimizations to improve training efficiency and model performance while keeping the codebase simple and easy to understand.

### quick start
下载代码仓库
```
git clone https://github.com/BV003/MicroMind
```
环境准备
```
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```
下载数据集，采用原项目提供的数据集
```
modelscope download --dataset gongjy/minimind_dataset
```

## 🛠️ Implementation

### Project Structure
```
.
├── README.md
├── dataset
│   ├── __init__.py
│   └── lm_dataset.py           # 用于加载和预处理训练数据（如分词、截断、构建输入输出格式）
├── eval_model.py               # 模型评估脚本，用于测试训练好的模型效果
├── model
│   ├── __init__.py
│   ├── model_lora.py           # LoRA（Low-Rank Adaptation）微调相关的模型实现，用于高效微调模型（减少参数量和计算量）
│   ├── model_minimind.py       # 核心模型定义，包含MiniMindConfig（模型配置类）和MiniMindForCausalLM（因果语言模型类），实现了 MiniMind 的基础结构
│   ├── tokenizer.json
│   └── tokenizer_config.json   # 自定义的minimind_tokenizer分词器文件，用于文本的编码和解码（词表大小 6400，保持模型轻量）
├── requirements.txt
├── scripts
│   ├── chat_openai_api.py      # 用于测试serve_openai_api.py启动的 API 服务的客户端脚本，验证接口功能
│   ├── convert_model.py        # 模型格式转换工具，可能用于将训练好的.pth模型转换为兼容transformers等框架的格式，便于部署和推理
│   ├── serve_openai_api.py     # 可将训练好的模型部署为 API 服务
│   ├── train_tokenizer.py      # 分词器训练脚本，用于基于数据集（如tokenizer_train.jsonl）训练自定义分词器
│   └── web_demo.py             # 基于 Streamlit 实现的极简聊天 WebUI，用于可视化交互测试模型效果
└── trainer
    ├── train_distill_reason.py # 推理能力蒸馏脚本，基于 DeepSeek-R1 等模型的蒸馏数据（如r1_mix_1024.jsonl）提升模型的推理能力
    ├── train_dpo.py            # 直接偏好优化（DPO）脚本，属于 RLHF（基于人类反馈的强化学习）阶段，使用dpo.jsonl数据集优化模型输出的偏好对齐
    ├── train_full_sft.py       # 全参数监督微调（SFT）脚本，使用 SFT 数据集（如sft_mini_512.jsonl）微调模型
    ├── train_lora.py           # LoRA 微调脚本，通过低秩适配技术高效微调模型（适用于特定领域如医疗、自我认知，使用lora_medical.jsonl等数据集）
    └── train_pretrain.py       # 预训练脚本，基于预训练数据集（如pretrain_hq.jsonl）对模型进行预训练
```

### model/model_minimind
- 定义了一个 模型配置类 (MiniMindConfig)，用于 存储和管理 MiniMind 模型的各种超参数和配置。在 Hugging Face Transformers 框架中，PretrainedConfig 类是一个标准基类，用于保存模型的结构参数（比如层数、隐藏维度、词表大小等），方便后续实例化模型或者加载预训练权重。
- 实现了 RMSNorm（Root Mean Square Layer Normalization），这是一种 Layer Normalization 的变种。
- 实现 MiniMind Transformer 的核心注意力机制，可处理长序列、多头、多专家场景。
- 实现了 Transformer 中的前馈网络（FeedForward, FFN）
- 实现了 Mixture-of-Experts (MoE) 模型中的门控（Gating）模块，是 MiniMind 中可选的 MoE 层核心逻辑
- 实现了 MiniMind 模型中的 Transformer Block，也就是模型的基本构建单元，每一层包含自注意力和前馈层（可选 MoE），类似于标准 Transformer，但做了一些轻量化和优化
- 实现了 MiniMind 模型的完整 Transformer 模型，即将多个 MiniMindBlock 堆叠起来，并加上输入嵌入、位置编码、最终归一化以及可选的 MoE 辅助损失计算
- 实现了 MiniMind 用于自回归语言建模（Causal LM）的封装，即把基础的 MiniMindModel 包装成可以直接用于生成任务（如文本生成、语言建模）的接口。

### model/model_lora
- LoRA：定义低秩模块
- apply_lora：在模型中插入 LoRA
- load_lora：加载 LoRA 权重
- save_lora：保存 LoRA 权重

### model/tokenizer
- 主要作用是定义模型如何将原始文本转换为可被模型理解的数字序列（tokens）
- 确保训练和推理阶段使用相同的分词逻辑，避免因文本转换方式不同导致的模型性能下降。例如，对多语言文本（如包含中英文字符的片段）进行一致的分割和编码。
- 使用token而不是字符的好处有，统一不同语言，降低序列长度，捕捉语义信息



### dataset/lm_dataset
- 定义了 4 个 Dataset 类，每个类对应 LLM 训练的不同阶段
- 把原始文本文件转换成模型训练所需的 token 张量
- 并且构建好输入（X）、目标输出（Y）、以及 loss 掩码（loss_mask）

### trainer/train_pretrain
- 实现了一个 带下限的余弦退火学习率调度器,以提高模型的收敛稳定性
- 执行一次训练轮（epoch）的所有训练步骤，包括：从 train_loader 读取 batch，前向计算模型输出（计算模型输出），计算损失，反向传播（backward计算梯度）与梯度累积，参数更新 (Step)，学习率动态调整，训练日志记录（logger & wandb），定期保存模型检查点

### trainer/train_full_sft
- 和pretrain相比较，使用 SFTDataset，在数据加载部分构造好指令
- 仅对助理回答部分计算损失（通过 loss_mask 控制）

### trainer/train_dpo
- logits 是模型最后一层（通常是线性层）的原始输出分数
- 实现dpo_loss,分别计算动态模型和参考模型关于正样本和负样本的概率差值，让这个差值作为loss，并且尽可能的大

### trainer/train_distill_reason
- 引入思考标签，特殊标签机制：<think> / <answer>，对那些特殊标记（<think>, <answer>等）赋予更高权重（惩罚系数=10）
- 基于RLHF模型进行的训练

### trainer/train_lora
- 只优化 LoRA 参数，冻结除了LoRA之外的其他所有参数


## 🧪 Experiment

### Pretrain

采用默认配置运行预训练
```
python train_pretrain.py
```
会加载pretrain_hq.jsonl中的数据进行预训练

在batch_size为32的情况下，训练用时50min,在batch_size为64的情况下，训练用时会减到30min


### SFT
调整batch_size为128，对应的学习率也扩大八倍

参数更新的公式可简化为
```
新参数 = 旧参数 - 学习率 × 梯度
```
大batch_size更新的轮次少，扩大学习率防止收敛过慢

运行代码
```
python train_full_sft.py
```



### RLHF

我们希望它能够更符合人的偏好，降低让人类不满意答案的产生概率。 
此处使用的是RLHF系列之-直接偏好优化(Direct Preference Optimization, DPO)。DPO通过推导PPO奖励模型的显式解，把在线奖励模型换成离线数据，Ref模型输出可以提前保存。DPO性能几乎不变，只用跑 actor_model 和 ref_model 两个模型，大大节省显存开销和增加训练稳定性。

```
python train_dpo.py
```

### LoRA (Low-Rank Adaptation)
相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。

```
python train_lora.py
```

如何使模型学会自己私有领域的知识？如何准备数据集？如何迁移通用领域模型打造垂域模型？ 这里举几个例子，对于通用模型，医学领域知识欠缺，可以尝试在原有模型基础上加入领域知识，以获得更好的性能。 同时，我们通常不希望学会领域知识的同时损失原有基础模型的其它能力，此时LoRA可以很好的改善这个问题。 


### Reasoning Model
一个遗憾是：参数太小的模型直接通过冷启动SFT+GRPO几乎不可能获得任何推理效果。这里只能采用蒸馏方法来实现Reasoning，只需要提前准备好相关的推理数据集即可。
```
python train_distill_reason.py
```


## 📌 Eval

### 测试模型
需要测试的模型*.pth文件位于./out/目录下，我们打算测试五个模型，模型的编号如下
```
# 0：测试pretrain
# 1：测试full_sft
# 2：测试rlhf
# 3：测试reason
# 4：测试lora(医学数据微调)+sft
```

### 问题

3个问题如下
```
如果A比B高，B比C矮，谁最高？
请写一个Python函数，用于计算一个列表中所有偶数的平方和。
请用简短的语言解释什么是“大语言模型”，并举一个应用例子。
```

### 模型回答

#### pretrain
```
👶: 如果A比B高，B比C矮，谁最高？
🤖️: 
A比C矮，B比C矮，B比C矮，A比C矮，A比C矮，C比C矮，A比C矮，A比C矮，C比C矮，B比C矮，C比C矮，C比C矮，C比C矮，A比C矮，A比C矮，A比C矮，C比C矮，A比C矮，C比C矮，A比C矮，B比C矮，C比C矮，A比C矮，B比C矮，A比C矮，A比C矮，C比C矮，A比C矮，A比C矮，C比C矮，B比C矮，C比C矮，A比C矮，C比C矮，A比C矮，C比C矮，C比C矮，C比C矮，A比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，A比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，A比C矮，A比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C矮，C比C比C矮，C比C矮，C比C矮，C比C矮，C比C多矮，A比C比C矮，C比C比C矮，A比C矮，A比C矮，A比C比C比C矮，A比C比C矮，C比C比C比C矮，C比C矮，C比C比C比C矮，C比C比C矮，C比C比C矮，C比C比C比C矮，C比C比C矮。
A比C矮，B比C比C比C比C矮，C比C矮，C比C比C比C比C矮，A比C多矮，A比C比C比C比C比C比C比C比C比C比C矮，C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比C比A比O比C比A比C比A比C比C比C比C比C比C比C比比C比A比C比C比A比C比比C比A比一比C比比A比。这是一个比C比C比C比B比的比B比B比。A比比C比是比C比C比C比C比A比为比A比C比C比A比A比比比C比A比C比C比B比不，C比C比C比C比C比A比C比A比不，C比C比比C比C比比，C比C比比比比B比比比C比C比A比比B比A比C比C比B比C比A比C比C比比C比C比C比C比C比A比A比B比C比C比B比B比A比A比A比B比C比C比C比A比B比C比B比。
比C比C比C比C比C比C比C比C比C比比C比B比C比C比比A比B比A比C比C比C比C比C比C比比C比C比C比B比C比比A比C比C比B比A比A比B比比C比比C比A比C比C比C比C比C比B比C比C比C比D比B比B比D比B比比B比C比C比A比比C比C比C比C比C比B比不比C比C比红比C比C比A比C比C比A比C比C比C比B比C比C比C比C比比C比C比C比C比C比B比B比比B比C比C比C比A比C比C比比C比C比C比C更比C比C比C比C比B比比C更比B比C比C比C比C比B多的比B比C比C比C比C比C比B比C比C比C比C比较比C比C比C和B比C比C比C比C矑比C比B比C比较比C比C比B比C比C比C比C比C比C比较比C比B比C比C比C比B比C比B比C比C比A比C�二比C比C比B比C更比C有C比C更比C更比C比C矑

👶: 请写一个Python函数，用于计算一个列表中所有偶数的平方和。
🤖️: 
以下是将列表中所有偶数的平方和的列表，可以使用Python或Java中的递归函数来实现一个简单的计算函数。
在编写一个简单的计算函数后，可以使用正则化测试函数来计算所有偶数的平方和。在此过程中，需要使用一些简单的函数来计算它们。这个函数会训练一个函数，用于计算所有偶数的平方和。函数将函数转换为任意数的平方和。函数将函数转换为整数，输出代数的平方和。
在编写Python函数时，需要进行代码调用。
对于一个包含大量偶数的平方和的变量，程序将函数转换为整数。其中，函数将函数转换为整数，输出代数。
在编写一个简单的计算函数时，需要使用正则化测试函数来确定每个偶数的平方和。在该函数中，函数将函数转换为整数，并将函数转换为数字，输出代数的平方和。
在编写Python函数时，需要使用Python中的递归函数来实现，并使用正则化测试函数。在此过程中，需要进行函数调用。
在编写Python函数时，需要进行代码调用，确保函数的逻辑与正则化验证。

👶: 请用简短的语言解释什么是“大语言模型”，并举一个应用例子。
🤖️: 
大语言模型是一种人工神经网络模型，用于解决各种问题，例如文本分类、语音识别和文本生成。大语言模型的训练可以被用于文本分类、命名实体识别、情感分析和文本生成等任务。在自然语言处理中，大语言模型主要用于文本分类、机器翻译、文本生成和机器翻译等任务。在自然语言处理中，大语言模型主要用于生成文本、语音和文本生成等任务。它在文本分类、命名实体识别、情感分析和文本生成等任务中都有应用。通过大规模数据集训练大语言模型，大型语言模型在文本分类、命名实体识别、情感分析和文本生成等任务中表现出色。通过训练大语言模型，可以实现文本分类、命名实体识别、情感分析和文本生成等任务。此外，大多数大型语言模型还被用于文本分类、命名实体识别、机器翻译、机器翻译、文本生成等任务。在自然语言处理中，大型语言模型已经被广泛应用于文本分类、命名实体识别、情感分析和文本生成等任务中。通过这些任务，大型语言模型可以更好地理解自然语言文本。
```
#### sft
```
👶: 如果A比B高，B比C矮，谁最高？
🤖️: 如果A比B高，B比C矮，B比C矮，那么A比C矮，B比C矮，因为A比C矮，B比C矮，这就是一般现象，如果A比C矮，C矮，那么A比C矮，那么B比C矮。
A比B矮，意味着A比B小，而C矮是两个数的乘积，因此A比C矮，B比C矮。
因此，A比C矮，B比C矮，因为B比C矮，所以A比C矮，C矮。
解题思路：
A：
- 假设A为A的对角线，B为A的对角线，B为A的对角线。
- 假设A为A的对角线，C为A的对角线，C为B的对角线。
- 假设A为A的对角线，C为A的对角线，A为A的对角线。
- 如果A为A的对角线，C为A的对角线，C为A的对角线，C为A的对角线，C为A的对角线。
- 如果A为A的对角线，C为A的对角线，C为A的对角线，C为A的对角线，C为A的对角线。
- 因此，B比C矮，C矮的对角线（A为A和B的对角线），C矮的对角线（A为A的对角线）。
- 因此，根据题目中的描述，B比C矮，A比C矮，C矮，C矮，C矮，A比C矮，C矮，A比C矮，A=D=C。
综上所述，根据题目的描述，A比C矮，B矮，C矮，A=D=C矮，C矮，A=D=C矮。

👶: 请写一个Python函数，用于计算一个列表中所有偶数的平方和。
🤖️: 为了帮助您，我需要您提供具体的列表，包括每个偶数和偶数的平方和。请提供包含所有偶数和偶数的列表，这样我才能帮助您进行排序。

👶: 请用简短的语言解释什么是“大语言模型”，并举一个应用例子。
🤖️: “大语言模型”指的是一种模型，它将文本分为多个类别，每类的文本都属于不同的主题。
1. **大语言模型**：大语言模型是一种在训练数据集上进行预测的模型，它将文本分为不同的主题，例如正面评价、负面评价或中性评价。
2. **数据类型**：大语言模型通常用于生成文本、图像或音频，如文本分类或情感分析。这意味着，在处理文本数据时，模型需要根据实际需求进行分析和处理，从而提高其准确性。
3. **模型类型**：大语言模型在模型的性能方面表现得相当不错，尤其是对于那些在特定场景下工作或学习中表现得很好。这通常需要大量的训练数据进行，以便进行准确的分类和预测。
4. **应用场景**：大语言模型在训练数据上表现较好，尤其是在处理文本时。这可能需要使用一些特定的应用程序，例如智能客服、自动驾驶汽车等。这通常需要使用一些在线资源，如在线课程、视频会议和在线社区等，以支持更准确的分类和预测。
5. **场景设计**：大语言模型通常用于构建模型、生成文本和进行场景设计。这通常需要一些技术支持，例如在自然语言处理、计算机视觉、语音识别和人工智能等领域。
```
#### rlhf
```
👶: 如果A比B高，B比C矮，谁最高？
🤖️: B比C矮，也被称为B，是因为在某些特定的上下文中，A可能指的是B，B通常指的是B，而C通常指的是B。然而，B更可能指的是B，因为C是B，而C是B，意味着B在某个特定的上下文中指的是B，但实际上是在特定的上下文中。

👶: 请写一个Python函数，用于计算一个列表中所有偶数的平方和。
🤖️: 为了帮助您，我需要一个具体的数字，请提供包含所有偶数的元素，例如字母，数字、字母，以及任何其他特殊字符。
请提供一个包含所有偶数的元素，例如字母、数字、字母、数字和特殊字符。这样我才能帮您计算出具体的列表中偶数的平方和。
如果您提供了一个包含所有偶数的元素，我将能帮您计算出最少偶数的值。请确保在指定列表中，只包含所有偶数的元素，且包含所有偶数。如果您没有特定的列表，请提供列表列表，我将为您计算所有偶数的值。
如果您没有特定的列表，请提供列表列表，我将帮助您计算出所有偶数的值。

👶: 请用简短的语言解释什么是“大语言模型”，并举一个应用例子。
🤖️: 大语言模型是AI领域的一个重要模型，它将输入与给定数据集相关的数据集合并，通过计算这些数据集的预设权，使其能够在不需要显式编程的情况下进行数据分类。
在大语言模型中，神经网络模型使用大量的输入数据，这些数据可以被用来预测数据集中的输出值，并通过处理这些数据来预测新的输出值。神经网络模型通常使用一些常见的算法，如朴素贝叶斯、支持向量机、决策树等，来构建模型，从而实现对数据的分类、预测、识别、推荐等任务。
在数据模型中，神经网络模型通常使用大量的训练数据来训练，以预测新的输出值，从而实现对新输出的预测。在实际应用中，神经网络模型可以用于预测和分类问题，而在决策树、支持向量机、朴素贝叶斯等神经网络中，神经网络模型可以用于分类、回归、预测、决策等任务。
```
#### reason
```
👶: 如果A比B高，B比C矮，谁最高？
🤖️: <think>
好，我现在要帮你想想B比C矮，B比C矮，B比C矮，谁最高。这是一个很常见的问题，因为B比C矮，但可能更偏向于A比C矮，但实际上没有特别的答案。
首先，B比C矮。B比C矮，因为B比C矮，而C矮。C比C矮，因为C比C矮，所以A比C矮，因为B比C矮，所以C矮，而不是B，所以B比C矮。
我记B比C矮，因为C矮，所以A比C矮，所以C矮，不像B矮。C矮不大，所以C矮，所以C矮。但是C矮，因为C矮，所以C矮矮。所以C矮的答案是B，C矮。
那么，B比C矮，因为C矮，所以C矮，所以C矮。但是C矮，因为C矮，所以C矮。C矮，因为C矮，所以C矮。B比C矮，因为C矮，所以C矮矮。
那么，C矮的答案是C矮，因为C矮，所以C矮。所以C矮矮的答案是C矮。
总的来说，C矮矮，所以C矮矮。
</think>
<answer>
C矮矮，因为C矮，所以C矮，所以C矮矮。
</answer>

👶: 请写一个Python函数，用于计算一个列表中所有偶数的平方和。
🤖️: <think>
嗯，用户想写一个Python函数，但在“列表中”中没有一个确切的因数，因为它在每一行都有一个数值。接下来，我需要确定每个偶数的平方和。
首先，我需要计算一个列表中所有偶数的平方和，然后计算该列表中所有偶数的平方和。然后，我需要计算每个偶数的平方和，以便求出每个偶数的平方和。
接下来，我需要确认一下每个偶数的平方和。如果每个偶数的平方和等于n，那么每个偶数的平方和等于n的平方和。
接下来，我需要判断每个偶数的平方和等于n的平方和。如果这是一个随机数，那么这是一个小数，且每个偶数的平方和等于n的平方。
接下来，我需要检查每个偶数是否有余数。如果没有余数，可能需要检查每一偶数的平方和等于n的平方和。
然后，我需要检查每个偶数的平方和等于n的平方和。确保每一偶数都为n的平方和，且每个偶数的平方和等于n的平方和。
最后，检查所有偶数的平方和等于n的平方和，确保每个偶数的平方和等于n的平方和。
</think>
<answer>
1. **列表中的每一偶数**：
1. **列表中的每一偶数**：
2. **列表中的每一偶数**：
1. **列表中的每一偶数**：
- **列表中的所有偶数**：
- **列表中的每一偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
2. **列表中的每一偶数**：
- **列表中的每一偶数**：
3. **列表中的每一偶数**：
4. **列表中的所有偶数**：
- **列表中的每一项偶数**：
5. **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：
- **列表中的所有偶数**：{列表中的所有偶数。

👶: 请用简短的语言解释什么是“大语言模型”，并举一个应用例子。
🤖️: <think>
好的，我现在需要用简短的语言解释一下什么是大语言模型。大语言模型是一种人工智能技术，旨在通过大规模语言模型对数据进行模式识别和预测。它由大量的文本数据组成，包括语音识别、自然语言处理、计算机视觉等。

大语言模型的核心思想是处理大量的文本数据，比如单词、短语、段落等。通过不断地学习和改进，大众可以有效地提高模型的准确性和效率。例如，通过使用大数据来预测目标变量，可以识别关键变量，并为新数据集提供预测结果。

举个例子，假设用户是在讨论大型语言模型，他们希望能够学习如何将大众的文本数据从一个小到小转变为另一个小的语言模型。这种模型通常涉及到大众的文本数据，比如文本、图片等，或者它们对数据进行建模和预测。

另一个例子是，使用大众语言模型，比如使用词嵌入、TF-IDF、GPT等，来训练和优化模型。这样，大众可以更好地理解和使用大众语言模型。

另一个例子是，大众语言模型通常使用大众语言模型，例如TF-IDF、GPT等，来处理大量的文本数据。这样，大众可以更快地获取大众的文本数据，从而更准确地预测目标变量。

另一个例子是，使用大众语言模型，比如GPT和GPT模型，可以预测目标变量的值。这样大众语言模型就能理解用户输入的问题，并提供相应的帮助。
</think>
<answer>
大众语言模型是一种用于处理大众语言数据的语言模型，主要用于处理大量的文本数据。它由大量的文本数据组成，这些数据来自大众的文本数据，通过大众语言模型，可以学习文本的特征和结构，从而进行预测和分析。
</answer>
```


#### lora(sft+medical_lora)
```
👶: 如果A比B高，B比C矮，谁最高？
🤖️: A的A比B高，也被称为AB，是一种常见的基因型疾病，主要在人体中包括基因型的细胞（ACDD或CRM）的功能。根据阿基米德的临床症状，DB和C矮，肿瘤是主要的蛋白质来源，主要是以蛋白质为主的细胞。
1. D的肿瘤：这意味着C的肿瘤可能会增加肿瘤的风险，从而影响肿瘤的功能。因此，B和C矮可能是B的。
2. 结核性：如果C的基因数量过多，可能会导致结核性疾病。结核性疾病通常包括DNA或RNA，这些基因可能会影响肿瘤的功能。例如，DNA在结核细胞中可以吸收或反复复制。
3. D的RNA或RNA。这意味着D的RNA可能会与D的DNA或RNA发生碱，形成类似于D的DNA分子。D的DNA分子通常会抵抗DNA的DNA。
4. D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的D的DD的D的D的D的D的D的D的D的D的D的D的D的DA的D的D的D的D的DD的D的D的D基因的D的DD基因。

👶: 请写一个Python函数，用于计算一个列表中所有偶数的平方和。
🤖️: 当然，我可以帮助你创建一个Python函数，用于计算列表中所有偶数的平方和。请提供你具体想我需要帮助的部分，例如，如何计算列表中的所有偶数和数字。如果你有任何疑问，我很乐意帮助你。请提供你的症状，包括你的姓名、联系方式、检查结果以及你的症状。
例如，如果你有一个数字和数字的平均数，你应该去看医生。如果你有任何疑问，我会尽力帮助你。

👶: 请用简短的语言解释什么是“大语言模型”，并举一个应用例子。
🤖️: 大语言模型是一种用于识别和理解人类语言模式的机器，用于处理语言的文本和语音。它通常由一些语言模型（如文本、语音和自然语言）和复杂的文本数据来进行训练，从而能够生成文本、故事、语言、自然语言等。
在训练数据集中，大语言模型可以识别语言中的语言模式，帮助人们更高效地进行文本分类和分析。在实际应用中，大语言模型可以从文本中提取特定的单词或短语，从而生成更易于理解的文本。
例如，如果大语言模型可以学习到人类语言模式，可以对语言进行分类，以便于预测和处理。如果机器的文本数据集可以学习到人类语言的特定模式，那么可以使用自然语言处理技术来识别语言中的语言模式，以提高语言的准确性和可读性。
然而，实际应用中，大语言模型可能会被视为一种技术，因为它可以从人类的语言数据中学习到新的知识。这是一种潜在的风险，需要得到进一步的监管和控制。
```

### 打分
我们使用大模型来对模型的回答进行打分，结果如下表
| 模型编号 | 语言理解与生成 (0-10) | 知识与推理能力 (0-10) | 效率与资源 (0-10) | 总分 (0-30) |
|----------|----------------------|----------------------|-----------------|-------------|
| 0  pretrain        |           3          |          2           |          3      |       8     |
| 1  full_sft      |           4          |          3           |          3      |       10    |
| 2  rlhf     |           4          |          4           |          3      |       11    |
| 3  reason     |           4          |          4           |          3      |       11    |
| 4  lora+sft      |           3          |          3           |          3      |       9     |



### 观察结论
- 整体模型分数都不高，由于参数限制，模型的语言能力并不强
- 从结果可以看到和pretrain相比较，sft,rlhf,reason的的分数都还是有所上涨，说明了后训练的有效性
- 在进行lora训练后，lora的回答很明显带有医学相关信息


### 使用wandb查看曲线

<img src="./images/wandb.png" alt="wandb" style="vertical-align: middle; width: auto; max-width: 100%;" />

- lr图：学习率呈现出持续下降的趋势，从初始的约 0.00055 左右，随着训练步数增加，逐渐降低到接近 0.00005。这种学习率的衰减通常是为了在训练后期使模型参数更新更稳定，更精准地收敛到最优解
- loss图：损失值整体是下降的，从最初的 8 左右逐步降低，虽然过程中有波动，但大致趋势是朝着更低的损失发展，说明模型在训练过程中对数据的拟合程度在不断提高
- epoch 图：每个 epoch 所需的时间从初始的约 70 左右，随着训练步数增加，逐渐降低到接近 0。这可能是因为随着训练进行，一些计算上的优化（比如缓存、批处理效率提升等）使得每个 epoch 的计算时间减少